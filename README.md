üß† Alzheimer's Disease Prediction with Neural Networks and Genetic Algorithms

This project aims to predict the likelihood of Alzheimer's disease in patients using machine learning, specifically a Neural Network (NN), and optimize its input features using a Genetic Algorithm (GA). It is divided into two main parts:

    Part A: Train and optimize a neural network model for Alzheimer's prediction.
    Part B: Use a genetic algorithm for feature selection based on the fixed, pre-trained NN from Part A.

The goal is to build a robust model for early Alzheimer's detection, reduce overfitting, and improve generalization by selecting the most relevant features.

üîç Key Features

‚úÖ Part A ‚Äì Neural Network-Based Prediction

    Data Preprocessing
        - Handles missing values, scaling (standardization or normalization), and outlier detection.

    Model Training & Architecture
        - A feedforward neural network with tunable hyperparameters.
        - Implemented in a modular fashion for experimentation.
        - NOTE: Future versions will transition this to an object-oriented approach for better modularity and extensibility.

    Hyperparameter Tuning
        - Hidden layer size
        - Learning rate
        - Momentum
        - Regularization
        - Data transformation method

    Cross-Validation & Evaluation
        - K-fold cross-validation
        - Metrics: Accuracy, Precision, Recall, F1-score, ROC-AUC

    Visualization & Reporting
        - Training/validation curves, confusion matrices, hyperparameter convergence plots

üß¨ Part B ‚Äì Genetic Algorithm for Feature Selection

Goal:
    Select the most informative subset of 34 features to reduce dimensionality while preserving (or improving) predictive performance.

Key Components:
    - Encoding: Each individual is a binary vector (length 34), where 1 = keep feature, 0 = drop.
    - Population Initialization: Randomly generate individuals representing different feature subsets.

    Fitness Function:
        - Based on the validation performance (cross-entropy loss or accuracy) of the fixed NN from Part A.
        - Penalizes solutions using too many features to balance model simplicity and performance.

    Selection Methods: Tournament selection, rank-based, or roulette wheel.
    Crossover Strategies: Single-point, multi-point, and uniform crossover.
    Mutation & Elitism: Introduces diversity and preserves best solutions.

Evaluation:
    - Tested over multiple configurations of population size, crossover/mutation probabilities.
    - Termination Criteria:
        ‚Ä¢ No improvement over N generations
        ‚Ä¢ <1% change in best fitness
        ‚Ä¢ Max generations reached

üìÅ Project Structure

Alzheimer-Detection-NN-GA-Model/
‚îú‚îÄ‚îÄ alzheimers_disease_data.csv
‚îú‚îÄ‚îÄ GA/
‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îú‚îÄ‚îÄ exercise02.py
‚îÇ   ‚îú‚îÄ‚îÄ individual*.py
‚îÇ   ‚îú‚îÄ‚îÄ population*.py
‚îÇ   ‚îî‚îÄ‚îÄ Project_Œ•Œù_2024-25_ŒúŒ≠œÅŒøœÇ-Œí.pdf
‚îú‚îÄ‚îÄ NN/
‚îÇ   ‚îú‚îÄ‚îÄ exercise01.py
‚îÇ   ‚îú‚îÄ‚îÄ bonus.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îú‚îÄ‚îÄ helpers.py
‚îÇ   ‚îú‚îÄ‚îÄ Project_Œ•Œù_2024-25_ŒúŒ≠œÅŒøœÇ-Œë.pdf
‚îÇ   ‚îú‚îÄ‚îÄ bonus_dir/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cross_validate.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ save.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ visualize.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Results/
‚îÇ   ‚îú‚îÄ‚îÄ modeling/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ architecture.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tuning.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ etc.
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessing.py
‚îÇ   ‚îú‚îÄ‚îÄ reporting/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ experiments.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ report_writer.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ result_saving.py
‚îÇ   ‚îî‚îÄ‚îÄ Results/
‚îÇ       ‚îú‚îÄ‚îÄ A2/
‚îÇ       ‚îî‚îÄ‚îÄ A3/
‚îú‚îÄ‚îÄ requirements.txt


‚öôÔ∏è How It Works

üß† Part A ‚Äì Neural Network

    Preprocessing
        - Cleans and scales data, handles outliers.

    Model Definition
        - Fully connected NN defined in architecture.py.

    Hyperparameter Optimization
        - Explored via grid/random search, results saved in Results/A2/ and A3/.

    Cross-Validation & Evaluation
        - Evaluates generalization performance using k-fold CV.

    Visualization & Logging
        - Accuracy/loss curves and performance summaries saved under Results/.

    ‚ö†Ô∏è Future Work:
        - Refactor NN pipeline using object-oriented programming principles for better structure and flexibility.

üß¨ Part B ‚Äì Genetic Algorithm

    Feature Encoding
        - Individuals are binary masks of features (length = 34).

    Fitness Evaluation
        - Applies fixed NN weights and computes validation accuracy or loss + feature penalty.

    GA Workflow
        - Initialize population
        - Evaluate fitness
        - Select, crossover, mutate
        - Track best individuals

    Convergence Curves & Plots
        - Average fitness vs. generation plots used for convergence analysis.

    Final Evaluation
        - Compare NN (GA-selected features) vs. full NN:
            ‚Ä¢ Accuracy
            ‚Ä¢ Generalization
            ‚Ä¢ Overfitting
            ‚Ä¢ Retraining effect using the entire dataset

‚ñ∂Ô∏è Running the Project

Install dependencies:

    pip install -r requirements.txt

Run NN pipeline (Part A):

    python3 NN/exercise01.py

Run optimized NN (bonus):

    python3 NN/bonus.py

Run GA for feature selection (Part B):

    python3 GA/exercise02.py

üìù Notes

- The NN uses weights obtained from full training (no retraining per GA individual) to speed up GA execution.
- All results, including performance metrics, convergence plots, and tuning logs, are saved under the appropriate Results/ subdirectories.
- Code is modular and organized by functionality for easy modification and testing.

üõ†Ô∏è To Do (Future Improvements)

- Transition entire codebase, especially NN pipeline, to an **Object-Oriented Programming (OOP)** structure.



