# ğŸ§  Alzheimer's Disease Prediction

This project aims to predict the likelihood of Alzheimer's disease in patients using machine learning. The model classifies whether a patient is likely to have Alzheimer's disease based on various patient data. The pipeline includes data preprocessing, model training, hyperparameter tuning, evaluation, and visualization of results â€” with the goal of building an effective predictive model for early detection of Alzheimerâ€™s disease.

---

## ğŸ” Key Features

- **Data Preprocessing**  
  - Cleans the dataset by handling missing values, scaling features, and detecting outliers.

- **Modeling**  
  - A neural network model is trained to classify Alzheimerâ€™s disease status based on patient data.

- **Hyperparameter Tuning**  
  - Number of Hidden Neurons  
  - Learning Rate  
  - Momentum  
  - Transformation Method (Standardization vs Normalization)  
  - Regularization Coefficient  

- **Evaluation**  
  - Metrics include Accuracy, Precision, Recall, F1-score, and ROC-AUC.

- **Visualization**  
  - Confusion matrices, training/validation accuracy and loss curves.

---

## ğŸ“ Project Structure

. â”œâ”€â”€ alzheimers_disease_data.csv # The dataset used for training the model. â”œâ”€â”€ config.py # Configuration file for model parameters. â”œâ”€â”€ exercise01.py # Main script to execute the project pipeline. â”œâ”€â”€ helpers.py # Helper functions for data handling and utilities. â”œâ”€â”€ modeling/ # Contains model architecture, training, evaluation, etc. â”‚ â”œâ”€â”€ architecture.py # Defines the neural network architecture. â”‚ â”œâ”€â”€ cross_validation.py # Implements cross-validation techniques. â”‚ â”œâ”€â”€ evaluation.py # Contains evaluation metrics and result logging. â”‚ â”œâ”€â”€ metrics.py # Functions to calculate various performance metrics. â”‚ â”œâ”€â”€ training.py # Implements model training logic. â”‚ â””â”€â”€ tuning.py # Tuning of hyperparameters like learning rate, momentum, etc. â”œâ”€â”€ preprocessing/ # Data preprocessing functions. â”‚ â”œâ”€â”€ preprocessing.py # Handles data cleaning, scaling, and outlier detection. â”œâ”€â”€ reporting/ # Logs and saves experiment results and metrics. â”‚ â”œâ”€â”€ experiments.py # Tracks the experiments and configuration. â”‚ â”œâ”€â”€ report_writer.py # Writes results and reports. â”‚ â””â”€â”€ result_saving.py # Saves model results, metrics, and plots. â”œâ”€â”€ visualization/ # Plots and charts for model evaluation and training progress. â”‚ â”œâ”€â”€ evalutation_plots.py # Plots related to model evaluation metrics. â”‚ â”œâ”€â”€ plot_base.py # Functions for saving and showing plots. â”‚ â””â”€â”€ training_plots.py # Plots the training and validation performance over epochs. â”œâ”€â”€ requirements.txt # Required dependencies for the project. â””â”€â”€ Results/ # Directory for storing results and output plots. â””â”€â”€ Standardization/ â”œâ”€â”€ accuracy_plot_fold_1.png â””â”€â”€ neural_network_results.txt


---

## âš™ï¸ How It Works

### 1. Data Preprocessing

- **Cleaning**: Handles missing values and removes unnecessary columns.  
- **Feature Scaling**: Applies either standardization or normalization depending on the selected method.  
- **Outlier Detection**: Uses Z-scores to detect and optionally handle outliers.

### 2. Model Architecture

- Defined in `architecture.py` with input, hidden, and output layers.
- The number of hidden neurons is a tunable hyperparameter.

### 3. Hyperparameter Tuning

- Optimizes the model for:
  - Number of hidden neurons
  - Learning rate
  - Momentum
  - Regularization coefficient
  - Transformation method

### 4. Cross-Validation

- Evaluates model generalization across multiple data splits.

### 5. Evaluation

- Uses Accuracy, Precision, Recall, F1-Score, and ROC-AUC to evaluate performance.
- Results are saved and logged for review.

### 6. Result Visualization

- Training progress and evaluation are visualized using:
  - Accuracy and loss plots
  - Confusion matrices
  - ROC curves (if implemented)

### 7. Results Reporting

- Final performance metrics, plots, and tuning logs are stored in the `Results/` directory.

---

## ğŸ“¦ Requirements

Install the required dependencies:

```bash
pip install -r requirements.txt

â–¶ï¸ Running the Project

To execute the entire pipeline (preprocessing, training, evaluation, and visualization), run:

python exercise01.py

ğŸ“Š Output & Results

After execution, the Results/ directory will contain:

    Accuracy and loss plots for each fold during cross-validation.

    Final evaluation metrics summary.

    Hyperparameter tuning logs and model configuration.

    Plots for model training and evaluation performance.
