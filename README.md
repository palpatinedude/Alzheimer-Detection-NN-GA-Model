ğŸ§  Alzheimer's Disease Prediction with Neural Networks and Genetic Algorithms

This project aims to predict the likelihood of Alzheimer's disease in patients using machine learning, specifically a Neural Network (NN), and optimize its input features using a Genetic Algorithm (GA). It is divided into two main parts:

- Part A: Train and optimize a neural network model for Alzheimer's prediction.
- Part B: Use a genetic algorithm for feature selection based on the fixed, pre-trained NN from Part A.

The goal is to build a robust model for early Alzheimer's detection, reduce overfitting, and improve generalization by selecting the most relevant features.

---

Key Features

Part A â€“ Neural Network-Based Prediction

- Data Preprocessing
  Handles missing values, scaling (standardization or normalization), and outlier detection.

- Model Training & Architecture
  - Feedforward neural network with tunable hyperparameters.
  - Modular implementation for experimentation.
  - Note: Future versions will transition to an object-oriented approach for better modularity and extensibility.

- Hyperparameter Tuning
  - Hidden layer size
  - Learning rate
  - Momentum
  - Regularization
  - Data transformation method

- Cross-Validation & Evaluation
  - K-fold cross-validation
  - Metrics: Accuracy, Precision, Recall, F1-score, ROC-AUC

- Visualization & Reporting
  Training/validation curves, confusion matrices, hyperparameter convergence plots.

---

Part B â€“ Genetic Algorithm for Feature Selection

Goal:
Select the most informative subset of 34 features to reduce dimensionality while preserving (or improving) predictive performance.

Key Components:
- Encoding: Each individual is a binary vector (length 34), where 1 = keep feature, 0 = drop.
- Population Initialization: Randomly generate individuals representing different feature subsets.

Fitness Function:
- Based on validation performance (cross-entropy loss or accuracy) of the fixed NN from Part A.
- Penalizes solutions using too many features to balance model simplicity and performance.

Genetic Operators:
- Selection: Tournament, rank-based, or roulette wheel.
- Crossover: Single-point, multi-point, uniform crossover.
- Mutation & Elitism: Introduce diversity and preserve best solutions.

Evaluation:
- Tested with multiple configurations of population size, crossover/mutation probabilities.
- Termination criteria:
  - No improvement over N generations
  - Less than 1% change in best fitness
  - Max generations reached

---

Project Structure

```plaintext
Alzheimer-Detection-NN-GA-Model/
â”œâ”€â”€ alzheimers_disease_data.csv
â”œâ”€â”€ GA/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ exercise02.py
â”‚   â”œâ”€â”€ individual.py
â”‚   â”œâ”€â”€ population.py
â”‚   â””â”€â”€ Project_Î¥Î_2024-25_ÎœÎ­ÏÎ¿Ï‚-Î’.pdf
â”œâ”€â”€ NN/
â”‚   â”œâ”€â”€ exercise01.py
â”‚   â”œâ”€â”€ bonus.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ helpers.py
â”‚   â”œâ”€â”€ Project_Î¥Î_2024-25_ÎœÎ­ÏÎ¿Ï‚-Î‘.pdf
â”‚   â”œâ”€â”€ bonus_dir/
â”‚   â”‚   â”œâ”€â”€ cross_validate.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ save.py
â”‚   â”‚   â”œâ”€â”€ visualize.py
â”‚   â”‚   â””â”€â”€ Results/
â”‚   â”œâ”€â”€ modeling/
â”‚   â”‚   â”œâ”€â”€ architecture.py
â”‚   â”‚   â”œâ”€â”€ training.py
â”‚   â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”‚   â”œâ”€â”€ tuning.py
â”‚   â”‚   â””â”€â”€ etc.
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â””â”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ reporting/
â”‚   â”‚   â”œâ”€â”€ experiments.py
â”‚   â”‚   â”œâ”€â”€ report_writer.py
â”‚   â”‚   â””â”€â”€ result_saving.py
â”‚   â””â”€â”€ Results/
â”‚       â”œâ”€â”€ A2/
â”‚       â””â”€â”€ A3/
â”œâ”€â”€ requirements.txt
```

---

How It Works

Part A â€“ Neural Network

- Preprocessing
  Cleans and scales data, handles outliers.

- Model Definition
  Fully connected NN defined in architecture.py.

- Hyperparameter Optimization
  Explored via grid/random search; results saved in Results/A2/ and A3/.

- Cross-Validation & Evaluation
  Evaluates generalization performance using k-fold cross-validation.

- Visualization & Logging
  Accuracy/loss curves and performance summaries saved under Results/.

- Future Work:
  Refactor NN pipeline using object-oriented programming for better structure and flexibility.

---

Part B â€“ Genetic Algorithm

- Feature Encoding
  Individuals are binary masks of features (length = 34).

- Fitness Evaluation
  Applies fixed NN weights and computes validation accuracy or loss plus a feature penalty.

- GA Workflow:
  1. Initialize population
  2. Evaluate fitness
  3. Select, crossover, mutate
  4. Track best individuals


- Convergence Curves & Plots
  Average fitness vs. generation plots for convergence analysis.

- Final Evaluation
  Compare NN (GA-selected features) vs. full NN on:
  - Accuracy
  - Generalization
  - Overfitting
  - Retraining effect using the entire dataset

---

Running the Project

# Install dependencies
pip install -r requirements.txt

# Run NN pipeline (Part A)
python3 NN/exercise01.py

# Run optimized NN (bonus)
python3 NN/bonus.py

# Run GA for feature selection (Part B)
python3 GA/exercise02.py

---

Notes

- The NN uses weights obtained from full training (no retraining per GA individual) to speed up GA execution.
- All results, including performance metrics, convergence plots, and tuning logs, are saved under the appropriate Results/ subdirectories.
- Code is modular and organized by functionality for easy modification and testing.

---

To Do (Future Improvements)

- Transition entire codebase, especially NN pipeline, to an Object-Oriented Programming (OOP) structure.


---

If you want me to save this as a .md or text file, just let me know.



