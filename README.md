# ğŸ§  Alzheimer's Disease Prediction with Neural Networks and Genetic Algorithms

This project aims to predict the likelihood of Alzheimer's disease in patients using machine learning, specifically a Neural Network (NN), and optimize its input features using a Genetic Algorithm (GA). It is divided into two main parts:

- Part A: Train and optimize a neural network model for Alzheimer's prediction.
- Part B: Use a genetic algorithm for feature selection based on the fixed, pre-trained NN from Part A.

The goal is to build a robust model for early Alzheimer's detection, reduce overfitting, and improve generalization by selecting the most relevant features.

---

# Key Features

Part A â€“ Neural Network-Based Prediction

- Data Preprocessing
  Handles missing values, scaling (standardization or normalization), and outlier detection.

- Model Training & Architecture
  - Feedforward neural network with tunable hyperparameters.
  - Modular implementation for experimentation.
  - Note: Future versions will transition to an object-oriented approach for better modularity and extensibility.

- Hyperparameter Tuning
  - Hidden layer size
  - Learning rate
  - Momentum
  - Regularization
  - Data transformation method

- Cross-Validation & Evaluation
  - K-fold cross-validation
  - Metrics: Accuracy, Precision, Recall, F1-score, ROC-AUC

- Visualization & Reporting
  Training/validation curves, confusion matrices, hyperparameter convergence plots.

---

Part B â€“ Genetic Algorithm for Feature Selection

Goal:
Select the most informative subset of 34 features to reduce dimensionality while preserving (or improving) predictive performance.

Key Components:
- Encoding: Each individual is a binary vector (length 34), where 1 = keep feature, 0 = drop.
- Population Initialization: Randomly generate individuals representing different feature subsets.

Fitness Function:
- Based on validation performance (cross-entropy loss or accuracy) of the fixed NN from Part A.
- Penalizes solutions using too many features to balance model simplicity and performance.

Genetic Operators:
- Selection: Tournament, rank-based, or roulette wheel.
- Crossover: Single-point, multi-point, uniform crossover.
- Mutation & Elitism: Introduce diversity and preserve best solutions.

Evaluation:
- Tested with multiple configurations of population size, crossover/mutation probabilities.
- Termination criteria:
  - No improvement over N generations
  - Less than 1% change in best fitness
  - Max generations reached

---

# Project Structure

```plaintext
Alzheimer-Detection-NN-GA-Model/
â”œâ”€â”€ alzheimers_disease_data.csv
â”œâ”€â”€ GA
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ exercise02.py
â”‚   â”œâ”€â”€ experiment.py
â”‚   â”œâ”€â”€ ga.py
â”‚   â”œâ”€â”€ individual.py
â”‚   â”œâ”€â”€ plotting.py
â”‚   â”œâ”€â”€ population.py
â”‚   â”œâ”€â”€ Project_Î¥Î_2024-25_ÎœÎ­ÏÎ¿Ï‚-Î’.pdf
â”‚   â”œâ”€â”€ reporting.py
â”‚   â”œâ”€â”€ Results
â”‚   â”‚   â”œâ”€â”€ best_ga_model_summary.txt
â”‚   â”‚   â”œâ”€â”€ best_set.txt
â”‚   â”‚   â”œâ”€â”€ selected_features.txt
â”‚   â”‚   â”œâ”€â”€ SET1/
â”‚   â”‚   â”œâ”€â”€ SET2/
â”‚   â”‚   â”œâ”€â”€ SET3/
â”‚   â”‚   â”œâ”€â”€ SET4/
â”‚   â”‚   â”œâ”€â”€ SET5/
â”‚   â”‚   â”œâ”€â”€ SET6/
â”‚   â”‚   â”œâ”€â”€ SET7/
â”‚   â”‚   â”œâ”€â”€ SET8/
â”‚   â”‚   â””â”€â”€ SET9/
â”‚   â”‚   â”œâ”€â”€ SET10/
â”‚   â”œâ”€â”€ results_comparison
â”‚   â”œâ”€â”€ sometests
â”‚   â”‚   â”œâ”€â”€ individualTest.py
â”‚   â”‚   â””â”€â”€ populationTest.py
â”‚   â””â”€â”€ Î•ÏÎ³Î±ÏƒÏ„Î·ÏÎ¹Î±ÎºÎ®_Î†ÏƒÎºÎ·ÏƒÎ·_ÎœÎ­ÏÎ¿Ï‚_Î’_.pdf
â”œâ”€â”€ main.py
â”œâ”€â”€ NN
â”‚   â”œâ”€â”€ bonus_dir
â”‚   â”‚   â”œâ”€â”€ cross_validate.py
â”‚   â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ Results
â”‚   â”‚   â”œâ”€â”€ save.py
â”‚   â”‚   â”œâ”€â”€ tuning_hidden.py
â”‚   â”‚   â””â”€â”€ visualize.py
â”‚   â”œâ”€â”€ bonus.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ exercise01.py
â”‚   â”œâ”€â”€ helpers.py
â”‚   â”œâ”€â”€ modeling
â”‚   â”‚   â”œâ”€â”€ architecture.py
â”‚   â”‚   â”œâ”€â”€ cross_validation.py
â”‚   â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â”œâ”€â”€ training.py
â”‚   â”‚   â””â”€â”€ tuning.py
â”‚   â”œâ”€â”€ old.py
â”‚   â”œâ”€â”€ preprocessing
â”‚   â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ Project_Î¥Î_2024-25_ÎœÎ­ÏÎ¿Ï‚-Î‘.pdf
â”‚   â”œâ”€â”€ reporting
â”‚   â”‚   â”œâ”€â”€ experiments.py
â”‚   â”‚   â”œâ”€â”€ report_writer.py
â”‚   â”‚   â””â”€â”€ result_saving.py
â”‚   â”œâ”€â”€ Results
â”‚   â”‚   â”œâ”€â”€ A1/
â”‚   â”‚   â”œâ”€â”€ A2/
â”‚   â”‚   â”œâ”€â”€ A3/
â”‚   â”‚   â”œâ”€â”€ best_ann_hyperparameters.json
â”‚   â”‚   â”œâ”€â”€ best_ann_model.keras
â”‚   â”‚   â”œâ”€â”€ best_ann_model_summary.txt
â”‚   â”‚   â”œâ”€â”€ best_ann_model.weights.h5
â”‚   â”‚   â”œâ”€â”€ test_data.npz
â”‚   â”‚   â”œâ”€â”€ val_data.npz
â”‚   â”‚   â””â”€â”€ Î‘4
â”‚   â”œâ”€â”€ visualization
â”‚   â”‚   â”œâ”€â”€ evalutation_plots.py
â”‚   â”‚   â”œâ”€â”€ plot_base.py
â”‚   â”‚   â””â”€â”€ training_plots.py
â”‚   â””â”€â”€ Î•ÏÎ³Î±ÏƒÏ„Î·ÏÎ¹Î±ÎºÎ®_Î†ÏƒÎºÎ·ÏƒÎ·_ÎœÎ­ÏÎ¿Ï‚_Î‘_.pdf
â”œâ”€â”€ plot.py

â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ results_comparison/
â”œâ”€â”€ retrain.py
â””â”€â”€ save.py  
```

---

# How It Works

Part A â€“ Neural Network

- Preprocessing
  Cleans and scales data, handles outliers.

- Model Definition
  Fully connected NN defined in architecture.py.

- Hyperparameter Optimization
  Explored via grid/random search; results saved in Results/A2/ and A3/.

- Cross-Validation & Evaluation
  Evaluates generalization performance using k-fold cross-validation.

- Visualization & Logging
  Accuracy/loss curves and performance summaries saved under Results/.

- Future Work:
  Refactor NN pipeline using object-oriented programming for better structure and flexibility.

---

Part B â€“ Genetic Algorithm

- Feature Encoding
  Individuals are binary masks of features (length = 34).

- Fitness Evaluation
  Applies fixed NN weights and computes validation accuracy or loss plus a feature penalty.

- GA Workflow:
  1. Initialize population
  2. Evaluate fitness
  3. Select, crossover, mutate
  4. Track best individuals


- Convergence Curves & Plots
  Average fitness vs. generation plots for convergence analysis.

- Final Evaluation
  Compare NN (GA-selected features) vs. full NN on:
  - Accuracy
  - Generalization
  - Overfitting
  - Retraining effect using the entire dataset

---

# Running the Project

# Install dependencies
pip install -r requirements.txt

# Run NN pipeline (Part A)
python3 NN/exercise01.py

# Run optimized NN (bonus)
python3 NN/bonus.py

# Run GA for feature selection (Part B)
python3 GA/exercise02.py

# Run entire pipeline  both the Neural Network training/optimization and Genetic Algorithm feature selection in one workflow
python3 main.py 

# Run retrains the neural network model after GA has selected the optimal subset of features and the NN hyperparameters have been optimized.
python3 retrain.py
---

# Notes

- The NN uses weights obtained from full training (no retraining per GA individual) to speed up GA execution.
- All results, including performance metrics, convergence plots, and tuning logs, are saved under the appropriate Results/ subdirectories.
- Code is modular and organized by functionality for easy modification and testing.

---

# To Do (Future Improvements)

- Transition entire codebase, especially NN pipeline, to an Object-Oriented Programming (OOP) structure.


---





